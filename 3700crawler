#!/usr/bin/env python3

import argparse, socket, ssl, html.parser, urllib.parse, time

DEFAULT_SERVER = "proj5.3700.network"
DEFAULT_PORT = 443

# Class used to parse HTML pages
class MyHTMLParser(html.parser.HTMLParser):
    def __init__(self, server, port, flags):
        html.parser.HTMLParser.__init__(self)
        self.page_links = []
        self.server = server
        self.port = port
        self.get_flag = False
        self.flags = flags
        
    # Looks for links to other internal pages or secret flags 
    def handle_starttag(self, tag, attrs):
        if tag == 'a':
            for attr in attrs:
                if attr[0] == 'href' and self.is_valid_link(attr[1]):
                    self.page_links.append(attr[1])

        elif tag == 'h2':
            for attr in attrs:
                if attr[0] == 'class' and attr[1] == 'secret_flag':
                    self.get_flag = True
				
    # Used to help print out flags
    def handle_endtag(self, tag):
        self.get_flag = False

    # Prints out a secret flag
    def handle_data(self, data):
        if self.get_flag:
            self.flags.add(data[6:])
            print(data[6:])

    # Determines if a link is a valid link (links to an internal page)
    def is_valid_link(self, link):
        o = urllib.parse.urlparse(link)
        if (o.netloc == '' or o.netloc == self.server or o.netlock == self.server+':'+self.port):
            if ('fakebook' in o.path):
                return True
            else:
                return False
        else:
            return False


# Class used to crawl fakebook
class Crawler:

    def __init__(self, args):

        # Server information
        self.server = args.server
        self.port = args.port
        self.username = args.username
        self.password = args.password

        # Variables to store cookies
        self.session_id = ''
        self.csrf_token_header = ''
        self.csrf_token_body = ''
        self.post_request = ''

        # Variables to track secret flags, visited, and unvisited webpages
        self.in_queue_or_visited = set()
        self.frontier = set()
        self.flags = set()

        # Socket setup
        self.mysocket = ''
        self.context = ssl.create_default_context()

        # HTML parser
        self.html_parser = MyHTMLParser(self.server, self.port, self.flags)

    # Resets the connection after each message received by the server
    def reset_connection(self):
        self.mysocket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        self.mysocket.connect((self.server, self.port))
        self.wrapped_socket = self.context.wrap_socket(self.mysocket, server_hostname=self.server)

    # Gets the redirect URL to visit from a 302 response
    def get_redirect_url(self, data):
        extracted_url_start = data.index('Location:') + 10
        extracted_url_end = data.index('\r\n', extracted_url_start)
        extracted_url = data[extracted_url_start:extracted_url_end]
        new_url = extracted_url
        return new_url

    # Returns the status code from a response
    def get_status_code(self, data):
        return data[:data.index('\r\n')]

    # Sends a get request with a given URL
    def send_get_request(self, url):

        # Reset connection
        self.reset_connection()

        # Build get request
        request = 'GET ' + url + ' HTTP/1.1\r\n'
        request += 'Host: '+str(self.server)+'\r\n'
        request += 'Connection: close\r\n'

        # Add cookies to request if we have set them
        if (self.csrf_token_header != '' and self.session_id != ''):
            request+= 'Cookie: csrftoken='+str(self.csrf_token_header)+'; sessionid='+(self.session_id)+'\r\n'

        # Finish request
        request += '\r\n'

        # Send request
        self.wrapped_socket.send(request.encode('ascii'))

        # Get response from server
        data = ''
        temp = 'temp'

        while (len(temp) != 0):
            temp = self.wrapped_socket.recv(10000).decode('ascii')
            data += temp

        # Get the status code, and set cookies if necessary
        status_code = self.get_status_code(data)
        self.update_cookies(data)

        # Depending on status code, return data or re-request 
        if '200' in status_code:
            return data
        elif '302' in status_code:
            html = self.get_html(data)
            new_url = self.get_redirect_url(data) 
            self.in_queue_or_visited.add(new_url)
            return self.send_get_request(new_url)
        elif '503' in status_code:
            return self.send_get_request(url)
        elif '403' in status_code or '404' in status_code:
            return -1
    

    # Send a post request with a given URL
    def send_post_request(self, url):

        # Reset the connection to the server
        self.reset_connection()

        # Build the post request
        body = 'username='+str(self.username)+'&password='+str(self.password)+'&csrfmiddlewaretoken='+str(self.csrf_token_body)+'&next=%2Ffakebook%2F'

        post_request = 'POST '+str(url)+ ' HTTP/1.1\r\n' 
        post_request += 'Host: '+str(self.server)+'\r\n'
        post_request += 'Cookie: csrftoken='+str(self.csrf_token_header)+'; sessionid='+str(self.session_id)+'\r\n'
        post_request += 'Content-type: application/x-www-form-urlencoded\r\n'
        post_request += 'Content-length: '+str(len(body))+'\r\n'
        post_request += 'Connection: close\r\n'
        post_request += '\r\n'
        post_request += body

        # Send the post request 
        self.wrapped_socket.send(post_request.encode('ascii'))

        # Get the response from the server
        data = ''
        temp = 'temp'

        while (len(temp) != 0):
            temp = self.wrapped_socket.recv(10000).decode('ascii')
            data += temp

        # Get the status code and update cookies
        status_code = self.get_status_code(data)
        self.update_cookies(data)

        # Either return data, re-request, or send a get request with redirect URL
        if '200' in status_code:
            return data
        elif '302' in status_code:
            new_url = self.get_redirect_url(data)
            self.in_queue_or_visited.add(new_url)
            return self.send_get_request(new_url)
        elif '503' in status_code:
            return self.send_post_request(url)

    # Update cookies for get requests if the response sends new cookies
    def update_cookies(self, response):
        try:
            session_id_start = response.index('sessionid') + 10
            session_id_end = response.index(';', session_id_start)
            self.session_id = response[session_id_start:session_id_end]

            csrf_start = response.index('csrftoken') + 10
            csrf_end = response.index(';', csrf_start)
            self.csrf_token_header = response[csrf_start:csrf_end]
        except:
            return

    # Get and update the csrfmiddlewaretoken field for the initial post request
    def update_csrfmiddlewaretoken(self, response):
        csrf_start = response.index('name=\"csrfmiddlewaretoken\"')+34
        csrf_end = response.index('>', csrf_start)-1
        self.csrf_token_body = response[csrf_start:csrf_end]

    # Login to the server
    def login(self):
        response = self.send_get_request('/accounts/login/?next=/fakebook/')
        self.update_csrfmiddlewaretoken(response)
        self.update_cookies(response)
        post_response = self.send_post_request('/accounts/login/?next=/fakebook/')
        return post_response

    # Gets the HTML from a GET response
    def get_html(self,response):
        body_idx = response.index('<!DOCTYPE')
        return response[body_idx:]
        
    # Explores the entire website
    def explore(self):
        while (len(self.frontier) != 0):
            response = self.send_get_request(self.frontier.pop())
            if (response == -1):
                continue
            html = self.get_html(response)
            self.add_links_and_scan(html)
            if (len(self.flags) == 5):
                return

    # Add links to the frontier, and scans web pages for secret flags
    def add_links_and_scan(self, html):
        self.html_parser.feed(html)
        for link in self.html_parser.page_links:
            if link not in self.in_queue_or_visited:
                self.frontier.add(link)
                self.in_queue_or_visited.add(link)
        self.html_parser.page_links = []

    # Runs the web scraper 
    def run(self):
        response = self.login()
        html = self.get_html(response)
        self.add_links_and_scan(html)
        self.explore()

# Entry point
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='crawl Fakebook')
    parser.add_argument('-s', dest="server", type=str, default=DEFAULT_SERVER, help="The server to crawl")
    parser.add_argument('-p', dest="port", type=int, default=DEFAULT_PORT, help="The port to use")
    parser.add_argument('username', type=str, help="The username to use")
    parser.add_argument('password', type=str, help="The password to use")
    args = parser.parse_args()
    sender = Crawler(args)
    sender.run()
